<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Mark @ Mozilla]]></title>
  <link href="http://mreid-moz.github.io/atom.xml" rel="self"/>
  <link href="http://mreid-moz.github.io/"/>
  <updated>2013-10-02T15:21:06-07:00</updated>
  <id>http://mreid-moz.github.io/</id>
  <author>
    <name><![CDATA[Mark Reid]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Deploying the next Telemetry Server]]></title>
    <link href="http://mreid-moz.github.io/blog/2013/10/02/deploying-the-next-telemetry-server/"/>
    <updated>2013-10-02T14:15:00-07:00</updated>
    <id>http://mreid-moz.github.io/blog/2013/10/02/deploying-the-next-telemetry-server</id>
    <content type="html"><![CDATA[<p>Yesterday was our target for cutting over to the &ldquo;rebooted&rdquo; telemetry server.
Despite some last-minute travel (I spent Monday en route from Nova Scotia to
San Francisco), I&rsquo;m happy to report that the switch went rather smoothly on
Tuesday! More details on the required changes can be found in <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=921161">Bug 921161</a>.</p>

<p>Since my last update, there have been a few last-minute code changes to get
things in ship-shape for deployment. The bulk of those changes were to the
scripts used to provision machines on Amazon&rsquo;s EC2 infrastructure, but there
was one more structural change of note.</p>

<p>The logic for processing incoming submissions (that&rsquo;s the &ldquo;validation,
conversion, and compression&rdquo; part) was previously controlled by a master
process which would launch a worker node to do the actual processing. Without
an easy way for masters to co-ordinate, it was difficult to launch extra
workers in cases where the rate of processing was not keeping up, since each
master expected its worker to process all available data.</p>

<p>The solution was to switch to using a <a href="http://aws.amazon.com/sqs/">queue</a> to keep track of data
available to be processed, and having the worker nodes claim data from the
queue. This results in a nicely decoupled architecture, where starting up more
workers (or killing off idle ones) is clean and easy.</p>

<p>Anyways, getting back to the main point&hellip; The cutover is complete, and the
Telemetry submissions are now going to &ldquo;The Cloud!&rdquo;</p>

<p>It turns out that the node.js version of the web server is efficient enough to
allow us to handle the entire volume of traffic using only a pair of &ldquo;t1.micro&rdquo;
nodes in EC2 (behind a load balancer).  Pretty slick.</p>

<p>So far, running on AWS has been pretty nice. The Elastic Load Balancers make it
nearly-trivial to add or remove nodes from the pool, and include useful (if
limited) monitoring. On the HTTP-serving nodes themselves, we have some more
detailed and application-specific monitoring using <a href="http://hekad.readthedocs.org/en/latest/">Heka</a>. The <a href="http://boto.s3.amazonaws.com/index.html">boto</a>
library makes it very easy to provision new nodes using python.</p>

<p>Now that the Telemetry Server is out in the wild, the next step is to get the
<a href="http://telemetry-dash.mozilla.org">new Dashboard</a> playing nicely with the new data source. Jonas Finnemann
Jensen is working on that.</p>

<p>There&rsquo;s still more work to do once the dashboard integration is complete,
including finishing up the C++ port of the &ldquo;process incoming&rdquo; code (which will
hopefully provide a large speedup compared to the current python
implementation), migrating the provisioning over to Amazon Cloud Formation,
creating a frontend for managing/running Telemetry MapReduce jobs, and
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=922745">exporting the historic data</a> from the previous Hadoop backend into the new
S3 storage.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Final Countdown]]></title>
    <link href="http://mreid-moz.github.io/blog/2013/09/17/the-final-countdown/"/>
    <updated>2013-09-17T14:05:00-07:00</updated>
    <id>http://mreid-moz.github.io/blog/2013/09/17/the-final-countdown</id>
    <content type="html"><![CDATA[<p>Release day is rapidly approaching. We&rsquo;re targeting October 1st for the release
of the first version of the telemetry reboot. Only two weeks left &ndash; exciting
times!</p>

<p>There&rsquo;s now a tracking bug that should list everything that needs to be done
before things go live: <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=911300">Bug 911300</a>.</p>

<p>The code has changed a fair bit since my last update, the most noticeable
modification being that the HTTP server is now using node.js instead of a
python+flask server.</p>

<p>Here is the basic architecture diagram for the system:
<img src="https://raw.github.com/mreid-moz/telemetry-server/master/docs/data_flow.png" alt="Telemetry Data Flow diagram" /></p>

<p>Data flows into an <code>incoming</code> bucket, which can then be processed separately by
one or more processing nodes, each of which publishes finalized data to a
<code>published</code> bucket.</p>

<p>The MapReduce code then reads from the <code>published</code> location for data analysis.</p>

<p>Other changes include updating the <a href="https://github.com/mreid-moz/telemetry-server/blob/master/process_incoming_mp.py">&ldquo;process incoming data&rdquo;</a> code to take
advantage of multiple processors, though it appears that python performance is
still a major bottleneck. Fortunately Mike Trinkala is working on porting the
conversion code to C++.</p>

<p>Keep an eye on the bug above for up-to-the-minute progress information, or as
always, feel free to drop by <code>#telemetry</code> on <code>irc.mozilla.org</code>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Two Years In]]></title>
    <link href="http://mreid-moz.github.io/blog/2013/08/08/two-years-on/"/>
    <updated>2013-08-08T11:47:00-07:00</updated>
    <id>http://mreid-moz.github.io/blog/2013/08/08/two-years-on</id>
    <content type="html"><![CDATA[<p>I figured since I missed it last year, I should post something on my
Mozillaversary!</p>

<p>I started on the Metrics team on August 8th, 2011, and more recently moved over
to the Performance team. Lots of fun stuff to learn and do, and I&rsquo;m happier
than ever that I joined Mozilla.</p>

<p>Two orders of business in this post. First, another status update on the
Telemetry Reboot project.</p>

<p>Things are basically feature-complete on the server now! Very exciting. The
high-altitude view looks like this:</p>

<p>Data comes in as HTTPS submissions, and is saved to disk in its raw form. These
files are converted to the new Histogram storage format, compressed, then sent
to Amazon S3 for permanent storage.</p>

<p>You can run MapReduce jobs on the S3 data (though at the moment there&rsquo;s a lot
more time spent on data transfer than I would like). If you run a MapReduce job
on the server node, you can also include up-to-the-minute data that has not yet
been exported to S3.</p>

<p>Now that the prototype (which reminds me a bit of this description of the
<a href="http://www.youtube.com/watch?feature=player_detailpage&amp;v=bzkRVzciAZg&amp;t=158">Hideous Creature</a> <strong>Warning:</strong> strong language) is working, the next step is to
benchmark things to make sure we can handle release-level volumes.</p>

<p>The second order of business is an interesting thing I learned about Python
File I/O that I thought I&rsquo;d share.</p>

<p>Normal file writes are not atomic, so if you have a situation where multiple
concurrent processes are appending lines to the same file, it&rsquo;s possible (and
in fact very likely) to end up with garbled lines. The Telemetry storage format
requires that each record be on its own line, so this caused real problems
during testing.</p>

<p>The solution: use <code>io.open(...)</code>.</p>

<p>For example, doing this will produce non-atomic writes:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>fout = open(filename, "a")
</span><span class='line'>fout.write(some_content)
</span><span class='line'>fout.close()</span></code></pre></td></tr></table></div></figure>


<p>So <code>some_content</code> could actually be written out in multiple disk operations,
with other writes by other processes in between.</p>

<p>Instead, to achieve atomic writes, you would use something like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>with io.open(filename, "a") as fout:
</span><span class='line'>    fout.write(some_content)</span></code></pre></td></tr></table></div></figure>


<p>Maybe this is common knowledge for Python folks, but I hope it helps.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Telemetry Server progress]]></title>
    <link href="http://mreid-moz.github.io/blog/2013/06/25/telemetry-server-progress/"/>
    <updated>2013-06-25T16:16:00-07:00</updated>
    <id>http://mreid-moz.github.io/blog/2013/06/25/telemetry-server-progress</id>
    <content type="html"><![CDATA[<p>Just a quick post to update on progress with the Telemetry Server project.</p>

<p>First and foremost, the <a href="https://github.com/mreid-moz/telemetry-server" title="Telemetry Server Repository">telemetry-server code is now on github</a>.</p>

<p>As of Friday, there is a basic prototype server up and running on an Amazon EC2 instance.</p>

<p>The prototype is able to accept submissions via HTTP, using URLs without or <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=860846">with dimension components</a>. Submissions are then converted to the new <a href="https://github.com/mreid-moz/telemetry-server/blob/master/StorageFormat.md" title="Storage Format">storage format</a> and saved to disk in the new <a href="https://github.com/mreid-moz/telemetry-server/blob/master/StorageLayout.md" title="On-disk Storage Layout">storage layout</a>.</p>

<p>As part of the conversion process, the histograms in the payload are validated against the correct revision of <code>Histograms.json</code>. This file is automatically fetched from the Mozilla mercurial server, then cached locally. The <code>RevisionCache</code> class encapsulates this logic.</p>

<p>The prototype server in its current form is too monolithic, and needs to be split up such that receiving submissions via HTTP is independent from the remainder of the data pipeline.</p>

<p>The first part, recieving and logging HTTP submissions, is a well-understood problem and there are many existing ways so solve it.  Some options are:</p>

<ul>
<li>Use the existing <a href="https://github.com/mozilla-metrics/bagheera" title="Bagheera">Bagheera</a> server, and consume messages from Kafka.</li>
<li>Make more direct use of one of the many message queues (such as Amazon SQS, RabbitMQ, or Kafka)</li>
<li>Write new code to save raw payloads to a temporary storage area using a similar layout to the long-term <a href="https://github.com/mreid-moz/telemetry-server/blob/master/StorageLayout.md" title="On-disk Storage Layout">storage layout</a>.</li>
</ul>


<p>The second part is already in place and working, with code that should be quite easy to pull out and run separately. One additional piece of functionality that would be nice is to calculate realtime stats before/while data is being persisted.</p>

<p>Other than separating receiving and processing functionality, the next major task will be building a <a href="https://github.com/mreid-moz/telemetry-server/blob/master/MapReduce.md" title="Telemetry MapReduce">MapReduce framework</a> to run on the Telemetry data. Initially, it will not be a full-blown cluster implementation, since reinventing that particular wheel is a huge task, but rather it will run on a single machine using multiple processes to parallelize map and reduce functionality.</p>

<p>One major advantage of the new <a href="https://github.com/mreid-moz/telemetry-server/blob/master/StorageLayout.md" title="On-disk Storage Layout">storage layout</a> is that MapReduce jobs will be able to filter the desired input data on a number of dimensions basically for free. Jobs that only need to look at a small subset of the data should be very efficient.</p>

<p>The first use case of the MapReduce framework is to produce the static data files for the new <a href="https://github.com/tarasglek/telemetry-frontend" title="Telemetry Frontend">telemetry frontend</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A New Project]]></title>
    <link href="http://mreid-moz.github.io/blog/2013/06/10/a-new-project/"/>
    <updated>2013-06-10T14:32:00-07:00</updated>
    <id>http://mreid-moz.github.io/blog/2013/06/10/a-new-project</id>
    <content type="html"><![CDATA[<p>Over the coming weeks I&rsquo;ll be embarking on a new project to revamp the acquisition, storage, and processing of <a href="https://wiki.mozilla.org/Telemetry">Mozilla Telemetry</a> data.</p>

<p>The general pipeline for Telemetry data looks like this:</p>

<ol>
<li>A Firefox user enables Telemetry in their browser</li>
<li>The browser generates performance data as it is used</li>
<li>Once a day, the browser submits the performance data to Mozilla via HTTPS</li>
<li>Data is accepted by <a href="https://wiki.mozilla.org/Telemetry">the server</a> and saved to a queue</li>
<li>The queue is polled for changes and payloads are saved to persistent storage</li>
<li>Analysis jobs are run against the persisted data, including daily aggregations</li>
<li>Results of analysis are visualized in <a href="https://wiki.mozilla.org/Telemetry">Telemetry Dashboards</a></li>
</ol>


<p>My initial focus will be on steps 5 and 6, specifically on improving the persistent storage format to be more space-efficient and to eliminate the need for re-processing to slice the data (by factors like Channel, Version, Day, etc).</p>

<p>I plan to post a link to the code repository here (as soon as there is something useful to share).</p>
]]></content>
  </entry>
  
</feed>
