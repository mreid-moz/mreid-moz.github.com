
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Mark @ Mozilla</title>
  <meta name="author" content="Mark Reid">

  
  <meta name="description" content="In a previous post, I described how to run an ad-hoc analysis on the
Telemetry data. There is often a need to re-run an analysis on an ongoing
basis &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://mreid-moz.github.io">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Mark @ Mozilla" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Mark @ Mozilla</a></h1>
  
    <h2>Performance team</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:mreid-moz.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/03/14/scheduling-telemetry-analysis/">Scheduling Telemetry Analysis</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-03-14T14:57:00-03:00" pubdate data-updated="true">Mar 14<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>In a <a href="http://mreid-moz.github.io/blog/2013/11/06/current-state-of-telemetry-analysis/">previous post</a>, I described how to run an ad-hoc analysis on the
Telemetry data. There is often a need to re-run an analysis on an ongoing
basis, whether it be for <a href="http://telemetry.mozilla.org">powering dashboards</a> or just to see how the
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=965707">data changes over time</a>.</p>

<p>We&rsquo;ve rolled out a new version of the
<a href="http://telemetry-dash.mozilla.org/">Self-Serve Telemetry Data Analysis Dashboard</a> with an improved interface
and some new features.</p>

<p>Now you can schedule analysis jobs to run on a daily, weekly, or monthly basis
and publish results to a public-facing Amazon S3 bucket.</p>

<h2>Typical Scheduling</h2>

<p>Here&rsquo;s how I expect that the analysis-scheduling capability will normally be
used:</p>

<ol>
<li>Log in to <a href="http://telemetry-dash.mozilla.org/">telemetry-dash.mozilla.org</a></li>
<li>Launch an analysis worker in the cloud</li>
<li>Develop and test your analysis code</li>
<li>Create a wrapper script to:

<ul>
<li>Do any required setup (install python packages, etc)</li>
<li>Run the analysis</li>
<li>Put output files in a given directory relative to the script itself</li>
</ul>
</li>
<li>Download and save your code from the worker instance</li>
<li>Create a tarball containing all the required code</li>
<li>Test your wrapper script:

<ul>
<li>Launch a fresh analysis worker</li>
<li>Run your wrapper script</li>
<li>Verify output looks good</li>
</ul>
</li>
<li>Head back to <a href="http://telemetry-dash.mozilla.org/">the analysis dashboard</a>, and schedule your analysis to run
as needed</li>
</ol>


<h2>Dissecting the &ldquo;Schedule a Job&rdquo; form</h2>

<ul>
<li><strong>Job Name</strong> is a unique identifier for your job. It should be a short,
descriptive string. Think &ldquo;what would I call this job in a code repo or
hostname?&rdquo; For example, the job that runs the data export for the
<a href="http://telemetry.mozilla.org/slowsql">SlowSQL dashboard</a> is called <code>slowsql</code>. You can add your username to
create a unique job name if you like (ie. <code>mreid-slowsql</code>).</li>
<li><strong>Code Tarball</strong> is the archive containing all the files needed to run your
analysis. The machine on which the job runs is a bare-bones Ubuntu machine
with a few common dependencies installed (git, xz-utils, etc), and it is up
to your code to install anything extra that might be needed.</li>
<li><strong>Execution Commandline</strong> is what will be invoked on the launched server. It
is the entry point to your job. You can specify an arbitrary Bash command.</li>
<li><strong>Output Directory</strong> is the location of results to be published to S3.
Again, these results will be publicly visible.</li>
<li><strong>Schedule Frequency</strong> is how often the job will run.</li>
<li><strong>Day of Week</strong> for jobs running with <code>daily</code> frequency.</li>
<li><strong>Day of Month</strong> for jobs running with <code>monthly</code> frequency.</li>
<li><strong>Time of Day (UTC)</strong> is when the job will run. Due to the distributed
nature of the Telemetry <a href="https://raw.github.com/mreid-moz/telemetry-server/master/docs/data_flow.png">data processing pipeline</a>, there may be some
delay before the previous day&rsquo;s data is fully available. So if your job is
processing data from &ldquo;yesterday&rdquo;, I recommend using the default vaulue of
Noon UTC.</li>
<li><strong>Job Timeout</strong> is the failsafe for jobs that don&rsquo;t terminate on their own.
If the job does not complete within the specified number of minutes, it will
be forcibly terminated. This avoids having stalled jobs run forever (racking
up our AWS bill the whole time).</li>
</ul>


<h2>Example: SlowSQL</h2>

<p>A concrete example of an analysis job that runs using the same framework is the
<a href="https://github.com/mozilla/telemetry-server/tree/master/mapreduce/slowsql">SlowSQL export</a>. The <code>package.sh</code> script creates the code tarball for this
job, and the <code>run.sh</code> script actually runs the analysis on a daily basis.</p>

<p>In order to schedule the SlowSQL job using the above form, first I would run
<code>package.sh</code> to create the code tarball, then I would fill the form as follows:</p>

<ul>
<li><strong>Job Name</strong>: <code>slowsql</code></li>
<li><strong>Code Tarball</strong>: select <code>slowsql-0.3.tar.gz</code></li>
<li><strong>Execution Commandline</strong>: <code>./run.sh</code></li>
<li><strong>Output Directory</strong>: <code>output</code> &ndash; this directory is created in <code>run.sh</code> and
data is moved here after the job finishes</li>
<li><strong>Schedule Frequency</strong>: <code>Daily</code></li>
<li><strong>Day of Week</strong>: leave as default</li>
<li><strong>Day of Month</strong>: leave as default</li>
<li><strong>Time of Day (UTC)</strong>: leave as default</li>
<li><strong>Job Timeout</strong>: <code>175</code> &ndash; typical runs during development took around 2 hours,
so we wait just under 3 hours</li>
</ul>


<p>The <a href="https://s3-us-west-2.amazonaws.com/telemetry-public-analysis/slowsql/data/slowsql20140308.csv.gz">daily data files</a> are then published in S3 and can be used from the
Telemetry SlowSQL Dashboard.</p>

<h2>Beyond Typical Scheduling</h2>

<p>The job runner doesn&rsquo;t care if your code uses the <a href="https://raw.github.com/mreid-moz/telemetry-server/master/docs/data_flow.png">python MapReduce</a>
framework or your own hand-tuned assembly code. It is just a generalized way to
launch a machine to do some processing on a scheduled basis.</p>

<p>So you&rsquo;re free to implement your analysis using whatever tools best suit the
job at hand.</p>

<p>The sky is the limit, as long as the code can be packaged up as a tarball and
executed on a Ubuntu box.</p>

<p>The pseudo-code for the general job logic is</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>fetch <span class="nv">$code_tarball</span>
</span><span class='line'>tar xzvf <span class="nv">$code_tarball</span>
</span><span class='line'><span class="sb">`</span><span class="nv">$execution_commandline</span><span class="sb">`</span>
</span><span class='line'><span class="nb">cd</span> <span class="nv">$output_directory</span>
</span><span class='line'>publish --recursive ./ s3://public/<span class="nv">$job_name</span>/data/
</span></code></pre></td></tr></table></div></figure>


<h2>Published Output</h2>

<p>One final reminder: Keep in mind that the results of the analysis will be made
<strong>publicly available</strong> on Amazon S3, so be absolutely sure that the output from
your job does not include any sensitive data.</p>

<p>Aggregates of the raw data are fine, but it is very important not to output the
raw data.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/02/13/recent-telemetry-data-outages/">Recent Telemetry Data Outages</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-02-13T16:55:00-04:00" pubdate data-updated="true">Feb 13<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>There have been a few data outages in Telemetry recently, so I figured some followup and explanation are in order.</p>

<p>Here&rsquo;s a graph of the submission rate from the <code>nightly</code> channel (where most of the action has taken place) over the past 60 days:</p>

<p><img src="images/telemetry_nightly20140213.png" title="Nightly submission rate for the past 60 days" alt="Heka Graph of nightly submissions for the past 60 days" /></p>

<p>The x-axis is time, and the y-axis is the number of submissions per hour.</p>

<p>Points <strong>A</strong> (December 17) and <strong>B</strong> (January 8) are false alarms, and were just cases where the stats logging itself got interrupted, and so don&rsquo;t represent data outages.</p>

<p>Point <strong>C</strong> (January 16) is where it starts to get interesting. In that case, Firefox nightly builds stopped submitting Telemetry data due to a change in event handling when the Telemetry client-side code was <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=913070" title="Bug 913070 - Move TelemetryPing to a .jsm">moved from a .js file to a .jsm module</a>.  The resolution is described in <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=962153" title="Bug 962153 - Drop in telemetry submissions since 2014-01-15">Bug 962153</a>.  This outage resulted in missing data for nightly builds from January 16th through to January 22nd.</p>

<p>As shown on the above graph, the submission rate dropped noticeably, but not anywhere close to zero.  This is because not everyone on the nightly channel updates to the latest nightly as soon as it&rsquo;s available, so an interesting side-effect of this bug is that we can see a glimpse of the rate at which nightly users update to new builds.  In short, it looks like a large portion of users update right away, with a long tail of users waiting several days to update. The effect is apparent again as the submission rate recovers starting on January 22nd.</p>

<p>The second problem with submissions came at point <strong>D</strong> (February 1) as a result of changing the client Telemetry code to <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=839794" title="Bug 839794 - Use OS.File in Telemetry">use OS.File for saving submissions</a> to disk on shutdown. This resulted in a more gradual decrease in submissions, since the <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=967203" title="Bug 967203 - Telemetry doesn't save pending pings on shutdown">&ldquo;saved-session&rdquo; submissions were missing</a>, but &ldquo;idle-daily&rdquo; submissions were still being sent. This outage resulted in partial data loss for nightly builds from February 1st through to February 7th.</p>

<p>Both of these problems have been limited to the <code>nightly</code> channel, so the actual volume of submissions that were lost is relatively low. In fact, if you compare the above graph to the graph for all channels:</p>

<p><img src="images/telemetry_all20140214.png" title="Overall submission rate for the past 60 days" alt="Heka Graph of all submissions for the past 60 days" /></p>

<p>The anomalies on January 16th and February 1st are not even noticeable within the usual weekly pattern of overall Telemetry submissions (we normally observe a sort of &ldquo;double peak&rdquo; each day, with weekends showing about 15-20% fewer submissions). This makes sense given that the <code>release</code> channel comprises the vast majority of submissions.</p>

<p>The above graphs are all screenshots of our <a href="http://hekad.readthedocs.org" title="Heka">Heka</a> aggregator instance. You can look at a <a href="http://people.mozilla.org/~mreid/telemetry/logs.aggregate.html" title="Aggregate Telemetry submission stats">live view of the submission stats</a> and explore for yourself. Follow the links at the top of the iframes to dig further into what&rsquo;s available.</p>

<p>There was a third data outage recently, but it cropped up in the server&rsquo;s data validation / conversion processing so it doesn&rsquo;t appear as an anomaly on the submission graphs. On February 4th, the <code>revision</code> field for many submissions started being rejected as invalid. The server code was expecting a <code>revision</code> URL of the form <code>http://hg.mozilla.org/...</code>, but was getting URLs starting with <code>https://...</code> and thus rejecting them. Since <code>revision</code> is required in order to determine the correct version of <code>Histograms.json</code> to use for validating the rest of the payload, these submissions were simply discarded. The change from <code>http</code> to <code>https</code> came from <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=960571" title="Bug 960571 - switch to https for build/test downloads and hg">a mostly-unrelated change to the Firefox build infrastructure</a>. This outage affected both nightly and aurora, and caused the loss of submissions from builds between February 4th and when the <a href="https://github.com/mozilla/telemetry-server/commit/1b8c1fb963d87dcccb9849f03a3b10587938e6ab" title="Add support for https urls in 'revision'">server-side fix</a> landed on Februrary 12th.</p>

<hr />

<p>So with all these outages happening so close together, what are we doing to fix it?</p>

<p>Going forward, we would like to be able to completely avoid problems like the overly-eager rejection of payloads during validation, and in cases where we can&rsquo;t avoid problems, we want to detect and fix them as early as possible.</p>

<p>In the specific case of the <code>revision</code> field, we are adding <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=972324" title="Bug 972324 - Test for changes of the revision value in payloads">a test that will fail if the URL prefix changes</a>.</p>

<p>In the case where the submission rate drops, we are adding <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=962811" title="Bug 962811 - Automatic e-mail notifications of Telemetry submission rate spikes &amp; drops">automatic email notifications</a> which will allow us to act quickly to fix the problem. The basic functionality is already in place thanks to Mike Trinkala, though the anomaly detection algorithm needs some tweaking to trigger on things like a gradual decline in submissions over time.</p>

<p>Similarly, if the <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=972887" title="Bug 972887 - Log and monitor server-side validation statistics">&ldquo;invalid submission&rdquo; rate</a> goes up in the server&rsquo;s validation process, we want to add <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=972889" title="Bug 972889 - Automatic e-mail notifications of Telemetry validation error rate spikes &amp; drops">automatic alerts</a> there as well.</p>

<p>With these improvements in place, we should see far fewer data outages in Q2 and beyond.</p>

<h3>Last minute update</h3>

<p>While poking around at various graphs to document this post, I noticed that <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=967203" title="Bug 967203 - Telemetry doesn't save pending pings on shutdown">Bug 967203</a> was still affecting <code>aurora</code>, so the fix has been uplifted.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/11/06/current-state-of-telemetry-analysis/">Current State of Telemetry Analysis</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-11-06T10:45:00-04:00" pubdate data-updated="true">Nov 6<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>The Telemetry Server has been deployed on AWS for just over a month now, so it&rsquo;s
time for an update.</p>

<p>The server code repository has been moved into the <a href="https://github.com/mozilla/telemetry-server"><em>Mozilla</em> github group</a>,
and the <code>mreid-moz</code> repo forwards there, so the change should be seamless.</p>

<p>The Telemetry dashboards have also moved! They are now located at
<a href="http://telemetry.mozilla.org">telemetry.mozilla.org</a>, nice and easy to remember.</p>

<p>Moving on to more interesting news, anyone with an <code>@mozilla.com</code> email can now
run their own Telemetry analysis jobs in the cloud. The procedure is still very
much in alpha/beta state, but if you&rsquo;ve got a question that can be answered
using Telemetry data, you&rsquo;re in luck.</p>

<p>Jonas has built <a href="https://github.com/mozilla/telemetry-server/blob/master/http/analysis-service/server.py">a mechanism for provisioning a ubuntu server as an Amazon EC2
instance</a>. These machines (<code>m2.4xlarge</code> in EC2 terms) have read-only
permission and a fast connection to Telemetry data stored in S3. Each machine
will be available for 24 hours, and will cost about $40USD to run. If you don&rsquo;t
need it for the full day, you can kill it early by following the instructions in
the webapp below.</p>

<p>Here&rsquo;s how it works:</p>

<ul>
<li>Visit the analysis provisioning dashboard at <a href="http://telemetry-dash.mozilla.org">telemetry-dash.mozilla.org</a>
and sign in using Persona (with an <code>@mozilla.com</code> email address as mentioned
above).</li>
<li>Enter some details. The <code>Server Name</code> field should be a short descriptive
name, something like &lsquo;mreid chromehangs analysis&rsquo; is good. Upload your
SSH public key (this allows you to log in to the server once it&rsquo;s started up)</li>
<li>Click <code>Submit</code>.</li>
<li>A Ubuntu machine will be started up on Amazon&rsquo;s EC2 infrastructure. Once it
has started up, you can SSH in and run your analysis job. Reload the webpage
after a couple of minutes to get the full SSH command you&rsquo;ll use to log in.</li>
<li><strong>Make sure to download your results</strong> when you&rsquo;re finished! Your analysis
machine will automatically get killed after 24 hours.</li>
</ul>


<p>Ok, that&rsquo;s all well and good, but what <em>is</em> an analysis job?</p>

<p>The easiest (and probably most familiar to anyone who has worked with Telemetry
data in the past) is to run a MapReduce job.</p>

<p>This requires a bit of setup on the machine you provisioned above:</p>

<ul>
<li>Clone the <code>telemetry-server</code> github repository:</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo apt-get install git
</span><span class='line'>$ git clone https://github.com/mozilla/telemetry-server.git</span></code></pre></td></tr></table></div></figure>


<ul>
<li>Set up some working directories:</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo mkdir /mnt/telemetry
</span><span class='line'>$ sudo chown ubuntu:ubuntu /mnt/telemetry
</span><span class='line'>$ mkdir /mnt/telemetry/work</span></code></pre></td></tr></table></div></figure>


<ul>
<li>Create an input filter to match the data you want. Look at the examples at
<code>mapreduce/examples/*.json</code>. Here is <a href="https://github.com/mozilla/telemetry-server/blob/master/mapreduce/examples/filter_saved_session_Fx_prerelease.json">a reasonably selective one</a>.</li>
<li>Create a new analysis script, or run one of the example ones:</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cd ~/telemetry-server
</span><span class='line'>$ python -m mapreduce.job mapreduce/examples/osdistribution.py \
</span><span class='line'>   --input-filter /path/to/filter.json \
</span><span class='line'>   --num-mappers 16 \
</span><span class='line'>   --num-reducers 4 \
</span><span class='line'>   --data-dir /mnt/telemetry/work \
</span><span class='line'>   --work-dir /mnt/telemetry/work \
</span><span class='line'>   --output /mnt/telemetry/my_mapreduce_results.out \
</span><span class='line'>   --bucket "telemetry-published-v1"</span></code></pre></td></tr></table></div></figure>


<p>A few notes for successful jobs.</p>

<ul>
<li>Try to keep the amount of data you&rsquo;re crunching to a minimum while you&rsquo;re
developing your job. It&rsquo;ll save time, and prevent the system from running out of
memory. A good start is to process just a single day&rsquo;s <code>nightly</code> data.</li>
<li>If you do run out of memory, try increasing the number of mappers and reducers.</li>
<li>After the first run, you can point the &ldquo;&mdash;data-dir&rdquo; argument at
<code>&lt;work-dir&gt;/cache</code> and add the &ldquo;&mdash;local-only&rdquo; parameter to skip downloading
files from S3 every time</li>
<li>Give us a heads-up in #telemetry and we&rsquo;ll tell you about any other caveats.</li>
</ul>


<p>One final note &ndash; the Telemetry MapReduce framework is a simple way to download
the set of records you are interested in, and <em>do something</em> for each record in
that data set.</p>

<p>If you don&rsquo;t want to do your analysis with this framework, you can just use it
to download the data (or even skip it altogether and download data using the AWS
command-line tools directly). Once the data has been downloaded to the machine,
you&rsquo;re free to analyze it using whatever other language / tools you&rsquo;re
comfortable with.</p>

<p>Happy Data Crunching!</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/10/02/deploying-the-next-telemetry-server/">Deploying the Next Telemetry Server</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-10-02T14:15:00-03:00" pubdate data-updated="true">Oct 2<span>nd</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Yesterday was our target for cutting over to the &ldquo;rebooted&rdquo; telemetry server.
Despite some last-minute travel (I spent Monday en route from Nova Scotia to
San Francisco), I&rsquo;m happy to report that the switch went rather smoothly on
Tuesday! More details on the required changes can be found in <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=921161">Bug 921161</a>.</p>

<p>Since my last update, there have been a few last-minute code changes to get
things in ship-shape for deployment. The bulk of those changes were to the
scripts used to provision machines on Amazon&rsquo;s EC2 infrastructure, but there
was one more structural change of note.</p>

<p>The logic for processing incoming submissions (that&rsquo;s the &ldquo;validation,
conversion, and compression&rdquo; part) was previously controlled by a master
process which would launch a worker node to do the actual processing. Without
an easy way for masters to co-ordinate, it was difficult to launch extra
workers in cases where the rate of processing was not keeping up, since each
master expected its worker to process all available data.</p>

<p>The solution was to switch to using a <a href="http://aws.amazon.com/sqs/">queue</a> to keep track of data
available to be processed, and having the worker nodes claim data from the
queue. This results in a nicely decoupled architecture, where starting up more
workers (or killing off idle ones) is clean and easy.</p>

<p>Anyways, getting back to the main point&hellip; The cutover is complete, and the
Telemetry submissions are now going to &ldquo;The Cloud!&rdquo;</p>

<p>It turns out that the node.js version of the web server is efficient enough to
allow us to handle the entire volume of traffic using only a pair of &ldquo;t1.micro&rdquo;
nodes in EC2 (behind a load balancer).  Pretty slick.</p>

<p>So far, running on AWS has been pretty nice. The Elastic Load Balancers make it
nearly-trivial to add or remove nodes from the pool, and include useful (if
limited) monitoring. On the HTTP-serving nodes themselves, we have some more
detailed and application-specific monitoring using <a href="http://hekad.readthedocs.org/en/latest/">Heka</a>. The <a href="http://boto.s3.amazonaws.com/index.html">boto</a>
library makes it very easy to provision new nodes using python.</p>

<p>Now that the Telemetry Server is out in the wild, the next step is to get the
<a href="http://telemetry-dash.mozilla.org">new Dashboard</a> playing nicely with the new data source. Jonas Finnemann
Jensen is working on that.</p>

<p>There&rsquo;s still more work to do once the dashboard integration is complete,
including finishing up the C++ port of the &ldquo;process incoming&rdquo; code (which will
hopefully provide a large speedup compared to the current python
implementation), migrating the provisioning over to Amazon Cloud Formation,
creating a frontend for managing/running Telemetry MapReduce jobs, and
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=922745">exporting the historic data</a> from the previous Hadoop backend into the new
S3 storage.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/09/17/the-final-countdown/">The Final Countdown</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-09-17T14:05:00-03:00" pubdate data-updated="true">Sep 17<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Release day is rapidly approaching. We&rsquo;re targeting October 1st for the release
of the first version of the telemetry reboot. Only two weeks left &ndash; exciting
times!</p>

<p>There&rsquo;s now a tracking bug that should list everything that needs to be done
before things go live: <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=911300">Bug 911300</a>.</p>

<p>The code has changed a fair bit since my last update, the most noticeable
modification being that the HTTP server is now using node.js instead of a
python+flask server.</p>

<p>Here is the basic architecture diagram for the system:
<img src="https://raw.github.com/mreid-moz/telemetry-server/master/docs/data_flow.png" alt="Telemetry Data Flow diagram" /></p>

<p>Data flows into an <code>incoming</code> bucket, which can then be processed separately by
one or more processing nodes, each of which publishes finalized data to a
<code>published</code> bucket.</p>

<p>The MapReduce code then reads from the <code>published</code> location for data analysis.</p>

<p>Other changes include updating the <a href="https://github.com/mreid-moz/telemetry-server/blob/master/process_incoming_mp.py">&ldquo;process incoming data&rdquo;</a> code to take
advantage of multiple processors, though it appears that python performance is
still a major bottleneck. Fortunately Mike Trinkala is working on porting the
conversion code to C++.</p>

<p>Keep an eye on the bug above for up-to-the-minute progress information, or as
always, feel free to drop by <code>#telemetry</code> on <code>irc.mozilla.org</code>.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/08/08/two-years-on/">Two Years In</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-08-08T11:47:00-03:00" pubdate data-updated="true">Aug 8<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>I figured since I missed it last year, I should post something on my
Mozillaversary!</p>

<p>I started on the Metrics team on August 8th, 2011, and more recently moved over
to the Performance team. Lots of fun stuff to learn and do, and I&rsquo;m happier
than ever that I joined Mozilla.</p>

<p>Two orders of business in this post. First, another status update on the
Telemetry Reboot project.</p>

<p>Things are basically feature-complete on the server now! Very exciting. The
high-altitude view looks like this:</p>

<p>Data comes in as HTTPS submissions, and is saved to disk in its raw form. These
files are converted to the new Histogram storage format, compressed, then sent
to Amazon S3 for permanent storage.</p>

<p>You can run MapReduce jobs on the S3 data (though at the moment there&rsquo;s a lot
more time spent on data transfer than I would like). If you run a MapReduce job
on the server node, you can also include up-to-the-minute data that has not yet
been exported to S3.</p>

<p>Now that the prototype (which reminds me a bit of this description of the
<a href="http://www.youtube.com/watch?feature=player_detailpage&amp;v=bzkRVzciAZg&amp;t=158">Hideous Creature</a> <strong>Warning:</strong> strong language) is working, the next step is to
benchmark things to make sure we can handle release-level volumes.</p>

<p>The second order of business is an interesting thing I learned about Python
File I/O that I thought I&rsquo;d share.</p>

<p>Normal file writes are not atomic, so if you have a situation where multiple
concurrent processes are appending lines to the same file, it&rsquo;s possible (and
in fact very likely) to end up with garbled lines. The Telemetry storage format
requires that each record be on its own line, so this caused real problems
during testing.</p>

<p>The solution: use <code>io.open(...)</code>.</p>

<p>For example, doing this will produce non-atomic writes:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>fout = open(filename, "a")
</span><span class='line'>fout.write(some_content)
</span><span class='line'>fout.close()</span></code></pre></td></tr></table></div></figure>


<p>So <code>some_content</code> could actually be written out in multiple disk operations,
with other writes by other processes in between.</p>

<p>Instead, to achieve atomic writes, you would use something like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>with io.open(filename, "a") as fout:
</span><span class='line'>    fout.write(some_content)</span></code></pre></td></tr></table></div></figure>


<p>Maybe this is common knowledge for Python folks, but I hope it helps.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/06/25/telemetry-server-progress/">Telemetry Server Progress</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-06-25T16:16:00-03:00" pubdate data-updated="true">Jun 25<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Just a quick post to update on progress with the Telemetry Server project.</p>

<p>First and foremost, the <a href="https://github.com/mreid-moz/telemetry-server" title="Telemetry Server Repository">telemetry-server code is now on github</a>.</p>

<p>As of Friday, there is a basic prototype server up and running on an Amazon EC2 instance.</p>

<p>The prototype is able to accept submissions via HTTP, using URLs without or <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=860846">with dimension components</a>. Submissions are then converted to the new <a href="https://github.com/mreid-moz/telemetry-server/blob/master/StorageFormat.md" title="Storage Format">storage format</a> and saved to disk in the new <a href="https://github.com/mreid-moz/telemetry-server/blob/master/StorageLayout.md" title="On-disk Storage Layout">storage layout</a>.</p>

<p>As part of the conversion process, the histograms in the payload are validated against the correct revision of <code>Histograms.json</code>. This file is automatically fetched from the Mozilla mercurial server, then cached locally. The <code>RevisionCache</code> class encapsulates this logic.</p>

<p>The prototype server in its current form is too monolithic, and needs to be split up such that receiving submissions via HTTP is independent from the remainder of the data pipeline.</p>

<p>The first part, recieving and logging HTTP submissions, is a well-understood problem and there are many existing ways so solve it.  Some options are:</p>

<ul>
<li>Use the existing <a href="https://github.com/mozilla-metrics/bagheera" title="Bagheera">Bagheera</a> server, and consume messages from Kafka.</li>
<li>Make more direct use of one of the many message queues (such as Amazon SQS, RabbitMQ, or Kafka)</li>
<li>Write new code to save raw payloads to a temporary storage area using a similar layout to the long-term <a href="https://github.com/mreid-moz/telemetry-server/blob/master/StorageLayout.md" title="On-disk Storage Layout">storage layout</a>.</li>
</ul>


<p>The second part is already in place and working, with code that should be quite easy to pull out and run separately. One additional piece of functionality that would be nice is to calculate realtime stats before/while data is being persisted.</p>

<p>Other than separating receiving and processing functionality, the next major task will be building a <a href="https://github.com/mreid-moz/telemetry-server/blob/master/MapReduce.md" title="Telemetry MapReduce">MapReduce framework</a> to run on the Telemetry data. Initially, it will not be a full-blown cluster implementation, since reinventing that particular wheel is a huge task, but rather it will run on a single machine using multiple processes to parallelize map and reduce functionality.</p>

<p>One major advantage of the new <a href="https://github.com/mreid-moz/telemetry-server/blob/master/StorageLayout.md" title="On-disk Storage Layout">storage layout</a> is that MapReduce jobs will be able to filter the desired input data on a number of dimensions basically for free. Jobs that only need to look at a small subset of the data should be very efficient.</p>

<p>The first use case of the MapReduce framework is to produce the static data files for the new <a href="https://github.com/tarasglek/telemetry-frontend" title="Telemetry Frontend">telemetry frontend</a>.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/06/10/a-new-project/">A New Project</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-06-10T14:32:00-03:00" pubdate data-updated="true">Jun 10<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Over the coming weeks I&rsquo;ll be embarking on a new project to revamp the acquisition, storage, and processing of <a href="https://wiki.mozilla.org/Telemetry">Mozilla Telemetry</a> data.</p>

<p>The general pipeline for Telemetry data looks like this:</p>

<ol>
<li>A Firefox user enables Telemetry in their browser</li>
<li>The browser generates performance data as it is used</li>
<li>Once a day, the browser submits the performance data to Mozilla via HTTPS</li>
<li>Data is accepted by <a href="https://wiki.mozilla.org/Telemetry">the server</a> and saved to a queue</li>
<li>The queue is polled for changes and payloads are saved to persistent storage</li>
<li>Analysis jobs are run against the persisted data, including daily aggregations</li>
<li>Results of analysis are visualized in <a href="https://wiki.mozilla.org/Telemetry">Telemetry Dashboards</a></li>
</ol>


<p>My initial focus will be on steps 5 and 6, specifically on improving the persistent storage format to be more space-efficient and to eliminate the need for re-processing to slice the data (by factors like Channel, Version, Day, etc).</p>

<p>I plan to post a link to the code repository here (as soon as there is something useful to share).</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2014/03/14/scheduling-telemetry-analysis/">Scheduling Telemetry Analysis</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/02/13/recent-telemetry-data-outages/">Recent Telemetry Data Outages</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/11/06/current-state-of-telemetry-analysis/">Current State of Telemetry Analysis</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/10/02/deploying-the-next-telemetry-server/">Deploying the Next Telemetry Server</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/09/17/the-final-countdown/">The Final Countdown</a>
      </li>
    
  </ul>
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 - Mark Reid -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
